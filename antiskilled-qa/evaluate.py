#!/usr/bin/env python3
"""
Antiskilled QA Evaluator
ä½¿ç”¨ Claude è¯„ä¼° Grok è§†é¢‘å¤„ç†è¾“å‡ºè´¨é‡
"""

import json
import sys
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from dataclasses import dataclass, asdict
import asyncio

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "Antiskilled"))

try:
    from openai import AsyncOpenAI
except ImportError:
    print("âŒ è¯·å®‰è£… openai: pip install openai")
    sys.exit(1)


@dataclass
class DimensionScore:
    """å•ä¸ªç»´åº¦è¯„åˆ†"""
    score: float  # 0-10
    issues: List[str]  # é—®é¢˜åˆ—è¡¨
    examples: List[str]  # å…·ä½“ç¤ºä¾‹


@dataclass
class QAResult:
    """å®Œæ•´è¯„ä¼°ç»“æœ"""
    video_id: str
    evaluated_at: str
    evaluator: str
    
    # 7 ç»´åº¦è¯„åˆ†
    accuracy: DimensionScore
    completeness: DimensionScore
    readability: DimensionScore
    signal_quality: DimensionScore
    hype_assessment: DimensionScore
    structural_quality: DimensionScore
    claims_quality: DimensionScore
    
    # æ€»åˆ†
    total_score: float
    grade: str  # A/B/C/D/F
    
    # ç»¼åˆåé¦ˆ
    recommendations: List[str]
    strengths: List[str]
    
    # å…ƒæ•°æ®
    evaluation_duration_seconds: float
    tokens_used: Optional[int] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬ä¸ºå­—å…¸"""
        result = asdict(self)
        # å°† DimensionScore å±•å¼€
        scores = {}
        issues = {}
        for dim in ['accuracy', 'completeness', 'readability', 'signal_quality', 
                    'hype_assessment', 'structural_quality', 'claims_quality']:
            dim_data = result.pop(dim)
            scores[f"{dim}_score"] = dim_data['score']
            issues[dim] = dim_data['issues']
        
        result['scores'] = scores
        result['issues'] = issues
        return result


class AntiskilledQAEvaluator:
    """QA è¯„ä¼°å™¨"""
    
    def __init__(
        self,
        api_key: str,
        model: str = "anthropic/claude-sonnet-4",
        base_url: str = "https://openrouter.ai/api/v1"
    ):
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url
        )
        self.model = model
    
    async def evaluate(
        self,
        transcript: str,
        audit_result: Dict[str, Any],
        video_id: Optional[str] = None
    ) -> QAResult:
        """
        è¯„ä¼°å•ä¸ªè§†é¢‘çš„ AI è¾“å‡ºè´¨é‡
        
        Args:
            transcript: åŸå§‹è½¬å½•æ–‡æœ¬
            audit_result: AI è¾“å‡º (audit_result.json)
            video_id: è§†é¢‘ IDï¼ˆå¯é€‰ï¼‰
        
        Returns:
            QAResult è¯„ä¼°ç»“æœ
        """
        start_time = datetime.now()
        
        # æ„å»ºè¯„ä¼° prompt
        prompt = self._build_evaluation_prompt(transcript, audit_result)
        
        # è°ƒç”¨ Claude
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {
                    "role": "system",
                    "content": self._get_system_prompt()
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.3,  # ä½æ¸©åº¦ä¿è¯ä¸€è‡´æ€§
            max_tokens=4000
        )
        
        # è§£æå“åº”
        result_text = response.choices[0].message.content
        result_data = self._parse_evaluation_result(result_text)
        
        # è®¡ç®—æ€»åˆ†å’Œç­‰çº§
        dimension_scores = [
            result_data['accuracy']['score'],
            result_data['completeness']['score'],
            result_data['readability']['score'],
            result_data['signal_quality']['score'],
            result_data['hype_assessment']['score'],
            result_data['structural_quality']['score'],
            result_data['claims_quality']['score']
        ]
        total_score = sum(dimension_scores) / len(dimension_scores)
        grade = self._calculate_grade(total_score)
        
        # æ„å»ºç»“æœ
        duration = (datetime.now() - start_time).total_seconds()
        
        qa_result = QAResult(
            video_id=video_id or audit_result.get('video_metadata', {}).get('video_id', 'unknown'),
            evaluated_at=datetime.now().isoformat(),
            evaluator=self.model,
            accuracy=DimensionScore(**result_data['accuracy']),
            completeness=DimensionScore(**result_data['completeness']),
            readability=DimensionScore(**result_data['readability']),
            signal_quality=DimensionScore(**result_data['signal_quality']),
            hype_assessment=DimensionScore(**result_data['hype_assessment']),
            structural_quality=DimensionScore(**result_data['structural_quality']),
            claims_quality=DimensionScore(**result_data['claims_quality']),
            total_score=round(total_score, 2),
            grade=grade,
            recommendations=result_data['recommendations'],
            strengths=result_data['strengths'],
            evaluation_duration_seconds=round(duration, 2),
            tokens_used=response.usage.total_tokens if response.usage else None
        )
        
        return qa_result
    
    def _get_system_prompt(self) -> str:
        """ç³»ç»Ÿ prompt"""
        return """ä½ æ˜¯ Antiskilled å¹³å°çš„ QA å®¡è®¡å‘˜ï¼Œè´Ÿè´£è¯„ä¼° AI ä»è´¢ç»è§†é¢‘ä¸­æå–çš„æ•°æ®è´¨é‡ã€‚

ä½ çš„ä»»åŠ¡æ˜¯æŒ‰ç…§ 7 ä¸ªç»´åº¦ä¸¥æ ¼è¯„åˆ†ï¼ˆ0-10 åˆ†ï¼‰ï¼Œæ‰¾å‡ºé—®é¢˜ï¼Œæä¾›æ”¹è¿›å»ºè®®ã€‚

è¯„åˆ†æ ‡å‡†ï¼š
- **å‡†ç¡®æ€§ (Accuracy)**: æ•°æ®ä¸åŸæ–‡ä¸€è‡´æ€§ï¼ˆticker, ä»·æ ¼, ç™¾åˆ†æ¯”, æ—¶é—´æˆ³ï¼‰
- **å®Œæ•´æ€§ (Completeness)**: æ˜¯å¦é—æ¼é‡è¦ä¿¡å·æˆ–è§‚ç‚¹
- **å¯è¯»æ€§ (Readability)**: è¯­è¨€è‡ªç„¶æµç•…ï¼Œæ— æœ¯è¯­å †ç Œ
- **ä¿¡å·è´¨é‡ (Signal Quality)**: conviction/action/reasoning æ˜¯å¦åˆç†
- **Hype è¯„ä¼° (Hype Assessment)**: hype_dimensions å„ç»´åº¦æ‰“åˆ†æ˜¯å¦å‡†ç¡®
- **ç»“æ„åŒ–è´¨é‡ (Structural Quality)**: summary_sections æ•°é‡/æ ‡é¢˜/highlight_tokens
- **Claims è´¨é‡ (Claims Quality)**: å¯éªŒè¯æ–­è¨€æå–å‡†ç¡®æ€§

è¾“å‡ºæ ¼å¼ï¼šä¸¥æ ¼ JSONï¼ŒåŒ…å«æ¯ä¸ªç»´åº¦çš„ score/issues/examplesï¼Œä»¥åŠ recommendations/strengthsã€‚

è¯„åˆ†åŠ¡å¿…å®¢è§‚ä¸¥æ ¼ï¼Œä¸è¦å› ä¸ºæ•´ä½“ä¸é”™å°±å…¨æ‰“é«˜åˆ†ã€‚"""

    def _build_evaluation_prompt(self, transcript: str, audit_result: Dict[str, Any]) -> str:
        """æ„å»ºè¯„ä¼° prompt"""
        # æå–å…³é”®æ•°æ®
        signals = audit_result.get('signals', [])
        summary_sections = audit_result.get('summary_sections', [])
        claims = audit_result.get('llm_response_processed', {}).get('claims', [])
        
        # æˆªæ–­è¿‡é•¿çš„ transcriptï¼ˆä¿ç•™å‰ 8000 å­—ç¬¦ï¼‰
        transcript_preview = transcript[:8000]
        if len(transcript) > 8000:
            transcript_preview += "\n\n[... è½¬å½•æ–‡æœ¬å·²æˆªæ–­ï¼Œä»…å±•ç¤ºå‰ 8000 å­—ç¬¦ ...]"
        
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ AI è§†é¢‘å¤„ç†è¾“å‡ºçš„è´¨é‡ã€‚

# åŸå§‹è½¬å½•æ–‡æœ¬
```
{transcript_preview}
```

# AI è¾“å‡ºæ•°æ®

## Signals ({len(signals)} ä¸ª)
```json
{json.dumps(signals, indent=2, ensure_ascii=False)}
```

## Summary Sections ({len(summary_sections)} ä¸ª)
```json
{json.dumps(summary_sections, indent=2, ensure_ascii=False)}
```

## Claims (å¦‚æœ‰)
```json
{json.dumps(claims, indent=2, ensure_ascii=False) if claims else "[]"}
```

## Hype Dimensions
```json
{json.dumps(signals[0].get('hype_dimensions', {}) if signals else {}, indent=2, ensure_ascii=False)}
```

---

è¯·æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼š

```json
{{
  "accuracy": {{
    "score": 9.0,
    "issues": ["ä»·æ ¼ $81.50 æ­£ç¡®ï¼Œä½† current_price ä¸ºå­—ç¬¦ä¸²è€Œé Decimal"],
    "examples": ["âœ… ROIC 21.8% å‡†ç¡®", "âŒ current_price åº”ä¸ºæ•°å­—ç±»å‹"]
  }},
  "completeness": {{
    "score": 8.5,
    "issues": ["é—æ¼æ¬¡è¦ ticker GOOGL åœ¨ Claims ä¸­"],
    "examples": ["âœ… ä¸»è¦ ticker UBER å®Œæ•´", "âš ï¸ secondary_tickers ä»…åœ¨ Signals ä¸­"]
  }},
  "readability": {{
    "score": 9.5,
    "issues": [],
    "examples": ["âœ… Summary æµç•…è‡ªç„¶", "âœ… æ— æœ¯è¯­å †ç Œ"]
  }},
  "signal_quality": {{
    "score": 8.0,
    "issues": ["conviction 0.85 ç•¥é«˜ï¼Œåšä¸»æœ‰ 'not top buy for 2026' ä¿ç•™"],
    "examples": ["âœ… action=BUY åˆç†", "âš ï¸ conviction å¯è°ƒè‡³ 0.75-0.8"]
  }},
  "hype_assessment": {{
    "score": 9.0,
    "issues": [],
    "examples": ["âœ… lexical=3.2 å‡†ç¡®ï¼ˆæ— ç…½åŠ¨è¯æ±‡ï¼‰", "âœ… certainty=4.8 åˆç†ï¼ˆæœ‰æ•°æ®æ”¯æ’‘ï¼‰"]
  }},
  "structural_quality": {{
    "score": 9.0,
    "issues": [],
    "examples": ["âœ… 7 ä¸ª sections åˆç†", "âœ… highlight_tokens æå–å‡†ç¡®"]
  }},
  "claims_quality": {{
    "score": 7.5,
    "issues": ["DCF $154 ç›®æ ‡ä»·æœªæå–ä¸ºç‹¬ç«‹ Claim"],
    "examples": ["âš ï¸ fair_value åº”åŒæ­¥ç”Ÿæˆ price_target Claim"]
  }},
  "recommendations": [
    "å°† current_price è½¬ä¸º Decimal ç±»å‹ä¿æŒä¸€è‡´æ€§",
    "ä» fair_value=$154 ç”Ÿæˆç‹¬ç«‹ Claim (claim_type='price_target')",
    "conviction å¾®è°ƒè‡³ 0.75-0.8 ä»¥åæ˜ åšä¸»ä¿ç•™æ€åº¦"
  ],
  "strengths": [
    "è´¢åŠ¡æŒ‡æ ‡ï¼ˆROIC, CAGRï¼‰æå–ç²¾å‡†",
    "é£é™©è®¨è®ºå…¨é¢ï¼ˆAV ç«äº‰, Tesla/Alphabetï¼‰",
    "Summary å¯è¯»æ€§å¼ºï¼Œæ— æœºå™¨å‘³"
  ]
}}
```

åŠ¡å¿…å®¢è§‚ä¸¥æ ¼ï¼Œä¸è¦å› ä¸ºæ•´ä½“ä¸é”™å°±å…¨æ‰“é«˜åˆ†ã€‚æ‰¾å‡ºæ‰€æœ‰å¯æ”¹è¿›ä¹‹å¤„ã€‚"""
        
        return prompt
    
    def _parse_evaluation_result(self, result_text: str) -> Dict[str, Any]:
        """è§£æ Claude è¿”å›çš„è¯„ä¼°ç»“æœ"""
        # æå– JSONï¼ˆå¯èƒ½è¢« ```json åŒ…è£¹ï¼‰
        import re
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', result_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # å°è¯•ç›´æ¥è§£ææ•´ä¸ªæ–‡æœ¬
            json_str = result_text
        
        try:
            data = json.loads(json_str)
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_dims = ['accuracy', 'completeness', 'readability', 'signal_quality',
                           'hype_assessment', 'structural_quality', 'claims_quality']
            for dim in required_dims:
                if dim not in data:
                    raise ValueError(f"Missing dimension: {dim}")
                if 'score' not in data[dim]:
                    raise ValueError(f"Missing score in {dim}")
            
            if 'recommendations' not in data:
                data['recommendations'] = []
            if 'strengths' not in data:
                data['strengths'] = []
            
            # ç¡®ä¿æ¯ä¸ªç»´åº¦æœ‰ issues å’Œ examples
            for dim in required_dims:
                if 'issues' not in data[dim]:
                    data[dim]['issues'] = []
                if 'examples' not in data[dim]:
                    data[dim]['examples'] = []
            
            return data
        
        except json.JSONDecodeError as e:
            print(f"âŒ JSON è§£æå¤±è´¥: {e}")
            print(f"åŸå§‹è¾“å‡º:\n{result_text}")
            raise
    
    def _calculate_grade(self, total_score: float) -> str:
        """è®¡ç®—ç­‰çº§"""
        if total_score >= 9.0:
            return 'A'
        elif total_score >= 7.0:
            return 'B'
        elif total_score >= 5.0:
            return 'C'
        elif total_score >= 3.0:
            return 'D'
        else:
            return 'F'


async def evaluate_video(
    transcript_path: Path,
    audit_result_path: Path,
    output_path: Path,
    api_key: str,
    model: str = "anthropic/claude-sonnet-4"
):
    """è¯„ä¼°å•ä¸ªè§†é¢‘"""
    print(f"ğŸ“Š è¯„ä¼°è§†é¢‘: {audit_result_path.name}")
    
    # è¯»å–æ•°æ®
    with open(transcript_path, 'r', encoding='utf-8') as f:
        transcript = f.read()
    
    with open(audit_result_path, 'r', encoding='utf-8') as f:
        audit_result = json.load(f)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = AntiskilledQAEvaluator(api_key=api_key, model=model)
    
    # æ‰§è¡Œè¯„ä¼°
    result = await evaluator.evaluate(transcript, audit_result)
    
    # è¾“å‡ºç»“æœ
    output_data = result.to_dict()
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æ‘˜è¦
    print(f"\n{'='*60}")
    print(f"ğŸ“¹ Video: {result.video_id}")
    print(f"â±ï¸  Duration: {result.evaluation_duration_seconds}s")
    print(f"ğŸ¯ Total Score: {result.total_score}/10 (Grade: {result.grade})")
    print(f"\nğŸ“ˆ Dimension Scores:")
    for dim in ['accuracy', 'completeness', 'readability', 'signal_quality',
                'hype_assessment', 'structural_quality', 'claims_quality']:
        score = output_data['scores'][f"{dim}_score"]
        issues_count = len(output_data['issues'][dim])
        status = "âœ…" if score >= 8.0 else "âš ï¸" if score >= 6.0 else "âŒ"
        print(f"  {status} {dim:20s}: {score}/10  ({issues_count} issues)")
    
    print(f"\nğŸ’¡ Key Recommendations:")
    for i, rec in enumerate(result.recommendations[:3], 1):
        print(f"  {i}. {rec}")
    
    print(f"\nâœ¨ Strengths:")
    for i, strength in enumerate(result.strengths[:3], 1):
        print(f"  {i}. {strength}")
    
    print(f"\nğŸ’¾ Report saved: {output_path}")
    print(f"{'='*60}\n")
    
    return result


async def batch_evaluate(
    video_dir: Path,
    output_dir: Path,
    api_key: str,
    model: str = "anthropic/claude-sonnet-4",
    min_score: float = 0.0,
    max_videos: Optional[int] = None
):
    """æ‰¹é‡è¯„ä¼°æ‰€æœ‰è§†é¢‘"""
    print(f"ğŸ” æ‰«æè§†é¢‘ç›®å½•: {video_dir}")
    
    # æŸ¥æ‰¾æ‰€æœ‰ audit_result.json
    audit_files = list(video_dir.glob("*/*_audit_result.json"))
    
    if not audit_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• audit_result.json æ–‡ä»¶")
        return
    
    if max_videos:
        audit_files = audit_files[:max_videos]
    
    print(f"ğŸ“¦ æ‰¾åˆ° {len(audit_files)} ä¸ªè§†é¢‘ï¼Œå¼€å§‹è¯„ä¼°...\n")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    results = []
    failed = []
    
    for i, audit_file in enumerate(audit_files, 1):
        video_id = audit_file.stem.replace('_audit_result', '')
        transcript_file = audit_file.parent / f"{video_id}_transcript.txt"
        
        if not transcript_file.exists():
            print(f"âš ï¸  [{i}/{len(audit_files)}] è·³è¿‡ {video_id}: ç¼ºå°‘ transcript")
            failed.append(video_id)
            continue
        
        output_file = output_dir / f"{video_id}_qa.json"
        
        try:
            result = await evaluate_video(
                transcript_file,
                audit_file,
                output_file,
                api_key,
                model
            )
            
            # ä»…ä¿å­˜ä½åˆ†è§†é¢‘
            if result.total_score < min_score:
                results.append(result)
            else:
                print(f"âœ… [{i}/{len(audit_files)}] {video_id}: {result.total_score}/10 (è¾¾æ ‡ï¼Œä¸ä¿å­˜)")
                output_file.unlink()  # åˆ é™¤é«˜åˆ†æŠ¥å‘Š
        
        except Exception as e:
            print(f"âŒ [{i}/{len(audit_files)}] è¯„ä¼°å¤±è´¥ {video_id}: {e}")
            failed.append(video_id)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary_file = output_dir / "summary.json"
    summary = {
        "evaluated_at": datetime.now().isoformat(),
        "total_videos": len(audit_files),
        "successful": len(results),
        "failed": len(failed),
        "failed_videos": failed,
        "low_score_videos": [
            {
                "video_id": r.video_id,
                "total_score": r.total_score,
                "grade": r.grade
            }
            for r in sorted(results, key=lambda x: x.total_score)
        ]
    }
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ‰¹é‡è¯„ä¼°å®Œæˆ")
    print(f"  æ€»è§†é¢‘æ•°: {len(audit_files)}")
    print(f"  æˆåŠŸè¯„ä¼°: {len(results)}")
    print(f"  å¤±è´¥: {len(failed)}")
    print(f"  ä½åˆ†è§†é¢‘ (<{min_score}): {len(results)}")
    print(f"\nğŸ“„ æ±‡æ€»æŠ¥å‘Š: {summary_file}")
    print(f"{'='*60}")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Antiskilled QA Evaluator - è¯„ä¼° AI è§†é¢‘å¤„ç†è´¨é‡"
    )
    
    # é€šç”¨å‚æ•°
    parser.add_argument(
        '--api-key',
        help='OpenRouter API Key (æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ OPENROUTER_API_KEY)',
        default=None
    )
    parser.add_argument(
        '--model',
        help='Claude æ¨¡å‹',
        default='anthropic/claude-sonnet-4'
    )
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å‘½ä»¤')
    
    # single å‘½ä»¤ï¼šè¯„ä¼°å•ä¸ªè§†é¢‘
    single_parser = subparsers.add_parser('single', help='è¯„ä¼°å•ä¸ªè§†é¢‘')
    single_parser.add_argument('--transcript', required=True, help='è½¬å½•æ–‡æœ¬è·¯å¾„')
    single_parser.add_argument('--audit-result', required=True, help='å®¡è®¡ç»“æœ JSON è·¯å¾„')
    single_parser.add_argument('--output', required=True, help='è¾“å‡ºè¯„ä¼°æŠ¥å‘Šè·¯å¾„')
    
    # batch å‘½ä»¤ï¼šæ‰¹é‡è¯„ä¼°
    batch_parser = subparsers.add_parser('batch', help='æ‰¹é‡è¯„ä¼°è§†é¢‘')
    batch_parser.add_argument('--video-dir', required=True, help='è§†é¢‘ç›®å½•ï¼ˆåŒ…å«å­æ–‡ä»¶å¤¹ï¼‰')
    batch_parser.add_argument('--output-dir', required=True, help='è¾“å‡ºç›®å½•')
    batch_parser.add_argument('--min-score', type=float, default=7.0, help='ä»…ä¿å­˜ä½äºæ­¤åˆ†æ•°çš„æŠ¥å‘Š')
    batch_parser.add_argument('--max-videos', type=int, help='æœ€å¤šè¯„ä¼°å¤šå°‘ä¸ªè§†é¢‘')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # è·å– API Key
    api_key = args.api_key or os.getenv('OPENROUTER_API_KEY')
    if not api_key:
        print("âŒ è¯·æä¾› API Key: --api-key æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ OPENROUTER_API_KEY")
        sys.exit(1)
    
    # æ‰§è¡Œå‘½ä»¤
    if args.command == 'single':
        asyncio.run(evaluate_video(
            Path(args.transcript),
            Path(args.audit_result),
            Path(args.output),
            api_key,
            args.model
        ))
    
    elif args.command == 'batch':
        asyncio.run(batch_evaluate(
            Path(args.video_dir),
            Path(args.output_dir),
            api_key,
            args.model,
            args.min_score,
            args.max_videos
        ))


if __name__ == '__main__':
    import os
    main()
